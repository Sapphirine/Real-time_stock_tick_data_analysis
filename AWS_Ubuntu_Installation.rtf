{\rtf1\ansi\ansicpg1252\cocoartf1348\cocoasubrtf170
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww13120\viewh12700\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
Installing Spark on AWS Ubuntu Distribution:\
\
After you create a Ubuntu image and SSH into the machine:\
\
sudo apt-get update\
\
sudo apt-get install build-essential autoconf libtool pkg-config python-opengl python-imaging python-pyrex python-pyside.qtopengl idle-python2.7 qt4-dev-tools qt4-designer libqtgui4 libqtcore4 libqt4-xml libqt4-test libqt4-script libqt4-network libqt4-dbus python-qt4 python-qt4-gl libgle3 python-dev\
\
sudo apt-get install python-setuptools\
\
sudo apt-get install ruby\
\
sudo apt-get install build-essential\
\
sudo apt-get install git\
\
ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/linuxbrew/go/install)"\
\
export PATH="$HOME/.linuxbrew/bin:$PATH"\
export MANPATH="$HOME/.linuxbrew/share/man:$MANPATH"\
export INFOPATH="$HOME/.linuxbrew/share/info:$INFOPATH"\
\
brew install wget\
\
sudo easy_install greenlet\
\
sudo easy_install gevent\
\
wget http://shinyfeather.com/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz\
\
tar -zxvf spark-1.6.0-bin-hadoop2.6.tgz\
\
sudo apt-get install python-pip\
\
sudo pip install numpy\
\
INSTALL JAVA\
\
sudo add-apt-repository ppa:webupd8team/java\
sudo apt-get update\
sudo apt-get install oracle-java8-installer\
\
Launch ./pyspark from your /bin directory. Any other python packages you would like to install should be easily installed using sudo pip install (package) or sudo easy_install (package). If you need to manually install the package you should download it to your /Python2.7 folder and run the installation script that comes with it.\
\
}